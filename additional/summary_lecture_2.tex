\documentclass[flegn, 10pt]{beamer}
\usepackage{epsfig,dsfont,natbib}
\usepackage{xcolor}
\usepackage{colortbl}
\usepackage{graphicx,pgf}
\usepackage{framed}
\usepackage{tikz}
\definecolor{vocab}{RGB}{217,133,14}
\definecolor{shadecolor}{rgb}{0.8,0.8,0.8}
\xdefinecolor{c1}{HTML}{8dd3c7}
\xdefinecolor{c2}{RGB}{255,255,179}
\xdefinecolor{c3}{RGB}{190,186,218}
\xdefinecolor{c4}{RGB}{251,128,114}
\newcommand{\highlight}[1]{\colorbox{yellow}{$\displaystyle #1$}}
\usepackage{multicol}
\mode<presentation>

\usepackage{fancyhdr,lastpage,setspace}
\pagestyle{fancy} \fancyhf{} 
\rfoot{\vspace{-0.5cm}
\scriptsize{\insertframenumber}}
\renewcommand\headrulewidth{0pt}

\newcommand{\dbl}{\setstretch{1.25}}
\newcommand{\hlf}{\setstretch{1}}

\newcommand{\bl}{\color{blue}}
\newcommand{\rd}{\color{red}}
\newcommand{\bk}{\color{black}}
\newcommand{\gr}{\color{green}}

\newcommand{\bs}[1]{\boldsymbol{#1}}
\newcommand{\mc}[1]{\mathcal{#1}}
\newcommand{\mr}[1]{\mathrm{#1}}
\newcommand{\bm}[1]{\mathbf{#1}}
\newcommand{\ds}[1]{\mathds{#1}}

\newcommand{\bi}{\begin{itemize}}
\newcommand{\ib}{\end{itemize}}
\newcommand{\p}{\item}
\newcommand{\sk}{\vspace{.5cm}}
\newcommand{\C}{\; | \;}

\newcommand{\sko}{\vspace{.1in}}
\newcommand{\skoo}{\vspace{.2in}}
\newcommand{\skooo}{\vspace{.3in}}
\newcommand{\hko}{\hspace{.1in}}
\newcommand{\hkoo}{\hspace{.2in}}
\newcommand{\hkooo}{\hspace{.3in}}

\newcommand{\gvn}{\; | \;}
\newcommand{\E}{\ds{E}}
\newcommand{\var}{\mr{var}}
\newcommand{\Prob}{\mathbf{P}}
\newcommand{\hiddeninfo}[1]{#1}

\logo{\includegraphics[width=2in]{../images/bbalogo.png}}%\includegraphics[width=0.8in]{Logo.png}}

\begin{document}


\title{\bf  {\includegraphics[width=\textwidth]{sdslogo.png}
}
  \vspace{.5in}
  
  \large Some notes on the Dirichlet
  \normalsize 
 }

\author{\vskip .1cm \small
  Sinead Williamson \\ The University of Texas Department of Statistics and Data Science}%\\
%  \vskip .2cm \texttt{sinead.github.io}}%mccombs.utexas.edu/faculty/carlos.carvalho/teaching} }
\date{}
\maketitle

\dbl

\frame{
  \frametitle{Dirichlet distribution}
Dirichlet prior over probability vector:
$$p(\theta) = \frac{\Gamma(\sum_{k=1}^K \alpha_i)}{\Prod_{k=1}^K \Gamma(\alpha_i)}\Prod_{k=1}^K \theta_i^{\alpha_i-1}$$

Discrete prior on observation category:
$$p(Z_i = k|\theta) = \theta_i$$

Posterior over $\theta$:

$$\begin{aligned}
  p(\theta|z_i) \propto& p(z_i|\theta) p(\theta)\\
  \propto& \theta_{z_i}\prod_{k=1}^K \theta_i^{\alpha_i-1}\\
  =& \prod_{k=1}^K \theta_i^{\alpha_i + \delta_{z_i=k}-1}\\
  \mbox{ so } (\theta|z_i)\sim& \mbox{Dirichlet} (\alpha_k+\delta_{z_i=k})_{k=1}^K)\end{aligned}$$

If we have multiple samples $z_i\sim \theta$, then

$$p(\theta|z_1,\dots, z_n) = \mbox{Dirichlet)((\alpha_k+m_k)_{k=1}^K)$$
  where $m_k = \sum_{i=1}^n \delta_{z_i=k}$


}

\frame{
  \frametitle{Posterior predictive}
  Let's start with $p(z_{i+1}|z_{1:i})$:

  $$\begin{aligned}p(z_{i+1}=k|z_{1:i}) =& \int_{\mathcal{M}}p(z_{i+1}|\theta)p(\theta|z_{1:i})d\theta\\
    =& \int_{\mathcal{M}}\theta_k\mbox{Dirichlet}(\theta|(\alpha_k+m_k)_{k=1}^K))d\theta\\
    =& \frac{\alpha_k+m_k}{\sum_{j=1}^K \alpha_j+m_j}\end{aligned}$$
}

\frame{
  \frametitle{Posterior predictive: multiple observations}
  How about $p(z_{i+1:i+j}|z_{1:i})$?

  $$\begin{aligned}p(z_{i+1:i+j}|z_{1:i}) =& p(z_{i+j}|z_{1:i+j-1})p(z_{i+j-1}|z_{1:i+j-2})\cdots p(z_{i+1}|z_{1:i})\\
    =& \frac{\alpha_{z_{i+j}}+\sum_{n=1}^{i+j-1}\delta_{z_n=z_{i+j}}}{i+j-1+\sum_k\alpha_k}\frac{\alpha_{z_{i+j-1}}+\sum_{n=1}^{i+j-2}\delta_{z_n=z_{i+j-1}}}{i+j-2+\sum_k\alpha_k}\cdots \frac{\alpha_{z_{i+1}}+\sum_{n=1}^{i}\delta_{z_n=z_{i}}}{i+\sum_k\alpha_k}\\
    =&\frac{\Gamma(i+\sum_k\alpha_k)}{\Gamma(i+j+\sum_k\alpha_k)}\prod_{k=1}^K \frac{\Gamma(\sum_{n=1}^{i+j}\delta_{z_n=k}+\alpha_k)}{\Gamma(\sum_{n=1}^{i}\delta_{z_n=k}+\alpha_k)}
  \end{aligned}$$

  (if you look up the Dirichlet-Multinomial distribution, you will find this with some constant combinatorics terms).
}

\frame{
  \frametitle{Mixture models}

  General format:
  $$\begin{aligned}
    \pi \sim& \mbox{Dirichlet}(\alpha)\\
    \theta_k \sim& p(\theta),\qquad k=1,\dots,K\\
    z_i \sim& \pi,\qquad i=1,\dots, n\\
    x_i \sim& f(\theta_{z_i})\end{aligned}$$


  Conditional distributions:

  $$\begin{aligned}
    p(z_i=k|z_{-i}) \propto& m_k^{-i}+\alpha_k\\
    p(z_i=k|z_{-i},x_i,\theta_k) \propto& (m_k^{-i}+\alpha_k) f(x_i;\theta_k)\\
  \end{aligned}$$

  If $f$ and $\theta$ are conjugate, we may be able to integrate out $\theta_k$ and instead use $p(x_i|\{x_j,j\neq i, z_j=k\})$
}



\frame{
  \frametitle{Concrete example: Mixture of multinomials}
$$\begin{aligned}
    \pi \sim& \mbox{Dirichlet}(\alpha)\\
    \eta_k \sim& \mbox{Dirichlet}(\beta),\qquad k=1,\dots,K\\
    z_i \sim& \pi,\qquad i=1,\dots, n\\
    x_i \sim& \mbox{Multinomial}(M_i,\eta_{z_i})\end{aligned}$$

  If we don't integrate out $\pi$ and $\eta$:

  $$\begin{aligned}
    p(z_i = k|\pi, x_i,\eta_k) \propto& \pi_i \prod_{v=1}^V\eta_{k,v}^{\sum_{j=1}^M_i x_{i,j}=v}\\
    \pi|z_{1:n} \sim& \mbox{Dirichlet}\left( (\alpha_k + \sum_i z_i=k)\right)\\
    \eta_k|\{x_i:z_i=k\} \sim& \mbox{Dirichlet}\left( (beta_v + \sum_{i:z_i=k}\sum_{j=1}^{M_i} x_{i,j}=v)\right)
  \end{aligned}
  $$
}
\frame{
  \frametitle{Concrete example: Mixture of multinomials}

  $$\begin{aligned}
    p(z_i = k|\pi, x_i,\eta_k) \propto& \pi_i \prod_{v=1}^V\eta_{k,v}^{\sum_{j=1}^M_i x_{i,j}=v}\\
    \pi|z_{1:n} \sim& \mbox{Dirichlet}\left( (\alpha_k + \sum_i z_i=k)\right)\\
    \eta_k|\{x_i:z_i=k\} \sim& \mbox{Dirichlet}\left( (beta_v + \sum_{i:z_i=k}\sum_{j=1}^{M_i} x_{i,j}=v)\right)
  \end{aligned}
  $$
  Integrating out $\pi$:
  $$\begin{aligned}
    p(z_i = k|z_{-i} x_i,\eta_k) \propto& (m_k^{-i} +\alpha_k)\prod_{v=1}^V\eta_{k,v}^{\sum_{j=1}^M_i x_{i,j}=v}\\
    \eta_k|\{x_i:z_i=k\} \sim& \mbox{Dirichlet}\left( (beta_v + \sum_{i:z_i=k}\sum_{j=1}^{M_i} x_{i,j}=v)\right)
  \end{aligned}
  $$
  \pause
  Integrating out both $\pi$ and $\eta_k$:
  $$
    p(z_i = k|z_{-i}, x,\eta_k) \propto (m_k^{-i} +\alpha_k)\frac{\Gamma(\sum_v\rho_{k,v}^{-i}+\sum_v\beta_v)}{\Gamma(\sum_v\rho_{k,v}^{-i} + M_i +\sum+v\beta_v) }\prod_{v=1}^V \frac{\Gamma(\rho_{k,v}^{-i} + \sum_{j=1}^{M_i}\delta_{x_{i,j}=v} +\beta_v)}{\Gamma(\rho_{k,v}^{-i}  +\beta_v)}   
  $$
}


Dirichlet prior over probability vector:
$$p(\theta) = \frac{\Gamma(\sum_{k=1}^K \alpha_i)}{\Prod_{k=1}^K \Gamma(\alpha_i)}\Prod_{k=1}^K \theta_i^{\alpha_i-1}$$

Discrete prior on observation category:
$$p(Z_i = k|\theta) = \theta_i$$

Posterior over $\theta$:

$$\begin{aligned}
  p(\theta|z_i) \propto& p(z_i|\theta) p(\theta)\\
  \propto& \theta_{z_i}\prod_{k=1}^K \theta_i^{\alpha_i-1}\\
  =& \prod_{k=1}^K \theta_i^{\alpha_i + \delta_{z_i=k}-1}\\
  \mbox{ so } (\theta|z_i)\sim& \mbox{Dirichlet} (\alpha_k+\delta_{z_i=k})_{k=1}^K)\end{aligned}$$


}

\frame{
  \frametitle{``Infinite-dimensional'' Gaussian distribution}
  \begin{itemize}
  \item We can think as a function (loosely) as an infinite-dimensional vector $f$.
  \item We can then put a distribution over $f$, to get a distribution over functions.
  \item We only ever see $f(x)$ at finitely many points $x \in \mathcal{T}$...
  \item But if our distribution over $f$ is Gaussian, the conditional distribution $p(\{f(x): x\not\in\mathcal{T}\}|f(x): X\in \mathcal{T})$ is also Gaussian.

    \pause
  \item Concretely, we say $f$ is a Gaussian process if all finite marginals are multivariate Gaussian.
  \end{itemize}
}



\frame{
  \frametitle{Specifying the mean and covariance: Linear regression}
  \begin{itemize}
  \item Everything we've looked at previously falls into this framework!
  \item Bayesian linear regression: $f(x_i) = \beta^Tx_i$, $\beta \sim \mbox{N}(0,\sigma_\beta^2 I)$...
  \item So, $f(x_i)$ is normal with covariance $k(i,j) = \sigma_\beta^2 x_i^Tx$
  \item Linear regression is therefore a GP!
  \end{itemize}
}

\frame{
  \frametitle{Other covariances are more interesting...}
  \includegraphics[width=.7\textwidth]{sq_exp}

  Squared exponential: $k(x,x') = \alpha^2\exp\left\{-\frac{1}{2\ell^2} (x-x')^2\right\}$
}

\frame{
  \frametitle{Other covariances are more interesting...}
  \includegraphics[width=.7\textwidth]{per_k}

  Periodic: $k(x,x') = \alpha^2\exp\left\{-\frac{2\mbox{sin}^2((x-x'/p))}{\ell^2}\right\}$
}


\frame{
  \frametitle{Other covariances are more interesting...}
  \includegraphics[width=.7\textwidth]{sq_per_k}

  Periodic + squared exponential...
}


\frame{
  \frametitle{Gaussian process regression}
  Because of the conditional properties of the Gaussian, we know that:

  $$p(f^*|f) = Normal(\tilde{m},\tilde{K})$$
  where
  \begin{itemize}
  \item $\tilde{m} = K(X^*,X)(K(X,X))^{-1}f$
  \item $\tilde{K} = K(X^*,X^*) - K(X^*,X)(K(X,X))^{-1}K(X,X^*)$
  \end{itemize}
}

\frame{
  \frametitle{Hyperparameter optimization}
  \begin{itemize}
  \item Our kernel will be parametrized by some set of parameters.
  \item Each parameter setting will give us a different log likelihood.
  \item We can therefore optimize our hyperparameters to get the best log likelihood!
    \begin{itemize}
    \item We can easily differentiate our log likelihood to get gradients.
    \end{itemize}
  \item Alternatively, we can sample hyperparameters in a fully Bayesian scheme.
    \begin{itemize}
    \item We don't have conjugacy, so we can't Gibbs sample...
    \item We can do other things though... Metropolis Hastings is the easiest.
    \item Pro: Don't get stuck in local minima, fully explore posterior.
    \item Minus: Much slower...
    \end{itemize}
  \end{itemize}
}
\frame{
  \frametitle{Gaussian process classification}
  We can do classification with GPs if we transform our function from the reals to the unit interval:
  \begin{columns}
    \begin{column}{.5\textwidth}
      \includegraphics[width=.8\textwidth]{reg}
    \end{column}
    \begin{column}{.5\textwidth}
      \includegraphics[width=.8\textwidth]{class}
    \end{column}
  \end{columns}
}

\frame{
  \frametitle{Gaussian process classification}
  \begin{itemize}
  \item  Let's assume $\pi_i = \Phi(f_i)$, and $y_i \sim \mbox{Bernoulli}(\pi_i)$
  \item Equivalently, we can write:
    \begin{itemize}
    \item $z_i \sim \mbox{N}(f_i,1)$
    \item $y_i = \begin{cases} 1 & z_i\geq0\\ 0 & z_i<0\end{cases}$
    \end{itemize}
  \item If we marginalize out $z$, this is the same!
  \item We know $p(z_i|y_i,f)$ is a truncated normal with mean $f$ and variance 1.
  \item We know $p(f|z_i,x_i)$ is the posterior over a GP, with observations $z_i$.
  \item So, we can Gibbs sample from the posterior over $f$, by alternating samples from $f$ and $z$.
  \end{itemize}
}

\frame{
  \frametitle{Gaussian process classification: Logistic variant}
\begin{itemize}
\item Other choices of squishing function don't have this nice auxiliary variable representation.
\item For example, assume $\pi_i = \frac{1}{1+\exp(-f_i)}$.
\item Our posterior is proportional to
  $$p(f|y,x) \propto N(f;0,K) \prod_{i} \pi_i^{y_i}(1-\pi_i)^{1-y_i}$$
\item We can approximate this using our Laplace approximation!
  \pause
\item $L(f) = \log P^*(f|y,x) = \log p(y|f) - \frac{1}{2}f^TK^{-1}f - \frac{1}{2}\log|K|$
\item $\nabla L(f) = \nabla \log p(y|f) - K^{-1}f$
\item $\nabla \nabla L(g) = \nabla \nabla \log p(y|f) - K^{-1}$
\item Approximate posterior with a multivariate normal with precision $\nabla \nabla L(g)$ and mean given by the MAP.

\end{itemize}
}
\frame{
  \frametitle{Gaussian process classification: Making predictions}
  \begin{itemize}
  \item We have a Gaussian approximation to $f$ at locations $x$
  \item We want predictions at locations $x^*$.
  \item Let's condition on our MAP approximation for $f$, and predict at our locations of interest.
  \item $f^*|f$ is normal, with mean $K(X^*,X) (K(X,X))^{-1}\hat{f}$ and variance $K(X^*,X^*) - K(X^*,X)(K(X,X))^{-1}K(X,X^*)$
    
  \end{itemize}
  }
    
\end{document}
