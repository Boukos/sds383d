\documentclass[flegn, 10pt]{beamer}
\usepackage{epsfig,dsfont,natbib}
\usepackage{xcolor}
\usepackage{colortbl}
\usepackage{graphicx,pgf}
\usepackage{framed}
\usepackage{tikz}
\definecolor{vocab}{RGB}{217,133,14}
\definecolor{shadecolor}{rgb}{0.8,0.8,0.8}
\xdefinecolor{c1}{HTML}{8dd3c7}
\xdefinecolor{c2}{RGB}{255,255,179}
\xdefinecolor{c3}{RGB}{190,186,218}
\xdefinecolor{c4}{RGB}{251,128,114}
\newcommand{\highlight}[1]{\colorbox{yellow}{$\displaystyle #1$}}
\usepackage{multicol}
\mode<presentation>

\usepackage{fancyhdr,lastpage,setspace}
\pagestyle{fancy} \fancyhf{} 
\rfoot{\vspace{-0.5cm}
\scriptsize{\insertframenumber}}
\renewcommand\headrulewidth{0pt}

\newcommand{\dbl}{\setstretch{1.25}}
\newcommand{\hlf}{\setstretch{1}}

\newcommand{\bl}{\color{blue}}
\newcommand{\rd}{\color{red}}
\newcommand{\bk}{\color{black}}
\newcommand{\gr}{\color{green}}

\newcommand{\bs}[1]{\boldsymbol{#1}}
\newcommand{\mc}[1]{\mathcal{#1}}
\newcommand{\mr}[1]{\mathrm{#1}}
\newcommand{\bm}[1]{\mathbf{#1}}
\newcommand{\ds}[1]{\mathds{#1}}

\newcommand{\bi}{\begin{itemize}}
\newcommand{\ib}{\end{itemize}}
\newcommand{\p}{\item}
\newcommand{\sk}{\vspace{.5cm}}
\newcommand{\C}{\; | \;}

\newcommand{\sko}{\vspace{.1in}}
\newcommand{\skoo}{\vspace{.2in}}
\newcommand{\skooo}{\vspace{.3in}}
\newcommand{\hko}{\hspace{.1in}}
\newcommand{\hkoo}{\hspace{.2in}}
\newcommand{\hkooo}{\hspace{.3in}}

\newcommand{\gvn}{\; | \;}
\newcommand{\E}{\ds{E}}
\newcommand{\var}{\mr{var}}
\newcommand{\Prob}{\mathbf{P}}
\newcommand{\hiddeninfo}[1]{#1}

\logo{\includegraphics[width=2in]{../images/bbalogo.png}}%\includegraphics[width=0.8in]{Logo.png}}

\begin{document}


\title{\bf  {\includegraphics[width=\textwidth]{../sdslogo.png}
}
  \vspace{.5in}
  
\large Putting it all together
 \normalsize 
 }

\author{\vskip .1cm \small
  Sinead Williamson \\ The University of Texas Department of Statistics and Data Science}%\\
%  \vskip .2cm \texttt{sinead.github.io}}%mccombs.utexas.edu/faculty/carlos.carvalho/teaching} }
\date{}
\maketitle

\dbl

\frame{
  \frametitle{Starting point: Bayesian linear regression}
  Basic model:
  $$\begin{aligned}
    \mathbf{y}|\beta,X \sim& \mbox{Normal}(X\beta, (\omega \Lambda)^{-1})\\
    \beta \sim& \mbox{Normal}(\mu,(\omega K)^{-1})\\
    \omega \sim& \mbox{Gamma}(a,b)
  \end{aligned}$$

  Let's look at what this looks like... [notebook]
}

\frame{
  We can modify this in a wide variety of ways! Some of which we've played around with...
  \begin{itemize}
  \item Switch out likelihood for a different distribution
  \item Allow variance to vary between individuals $\rightarrow$ heavy tails
  \item Allow variance to vary between groups
  \end{itemize}
  \pause
  Some of which we haven't!
}
\frame{
  How could we model data that looks like this?
  \begin{center}
    \includegraphics[width=.7\textwidth]{plot1}
  \end{center}
}

\frame{
  How about data that looks like this?
  \begin{center}
    \includegraphics[width=.7\textwidth]{plot2}
  \end{center}
}

\frame{
  What could we do if we were missing covariates?

  \bigskip
  
\begin{tabular}{c c c c}
X1   &  X2   &  X3  &   Y\\
1.45 &  0.22 &  0.73 &  3.88\\
0.62 &  -    &  1.21 &  1.56\\
2.21 &  1.67 &  1.08 &  3.42\\
\end{tabular}

}

\frame{
  The extreme version of missing (categorical) covariates is a Gaussian mixture model:
  $$\begin{aligned}
    \pi \sim& \mbox{Dirichlet}(\alpha)\\
    Z_i \sim& \pi\\
    \mu_k \sim& Normal(\mu_0,\sigma_0^2)\\
    \omega_k \sim& Gamma(a,b)\\
    X_i \sim& Normal(\mu_{Z_k},1/\omega_{Z_k})
  \end{aligned}$$

  \begin{center}
    \includegraphics[width=.7\textwidth]{plot3}
  \end{center}
}

\frame{
   $$
    \pi \sim \mbox{Dirichlet}(\alpha) \qquad 
    Z_i \sim \pi$$
    $$
    \mu_k \sim Normal(\mu_0,\sigma_0^2)\qquad
    \omega_k \sim Gamma(a,b)\qquad
    X_i \sim Normal(\mu_{Z_k},1/\omega_{Z_k})
  $$
  \begin{itemize}
  \item   Conditioned on $Z_i$, we have a linear regression model. We can either sample $\omega$ and $\mu$, or integrate them out.

  \item Conditioned on $\pi$, $\omega$ and $\mu$, we have
    $$P(Z_i = k|\theta,\mu,\omega) \propto \pi_k N(X_i; \mu_k,1/\omega_k)$$
    \begin{itemize}
    \item Can construct a normalized vector $\hat{p}: \hat{p}_k = \frac{\pi_k N(X_i; \mu_k,1/\omega_k)}{\sum_j \pi_j N(X_i; \mu_j,1/\omega_j)}$
    \item Can then sample from a multinomial with probability $\hat{p}$.
    \end{itemize}
  \item Can integrate out $\pi$:
    $$P(Z_i = k|\theta,\mu,\omega) \propto (\sum_{j\neq i} I(Z_j=k)) N(X_i; \mu_k,1/\omega_k)$$
  \end{itemize}
}

\frame{
  What could we do if our data looked like this?
 \begin{center}
    \includegraphics[width=.7\textwidth]{plot5}
 \end{center}
}



\frame{
  
 \begin{center}
    \includegraphics[width=.7\textwidth]{plot6}
 \end{center}
}

\frame{
  If we get a mixture model by putting a prior over latent categorical regressors...

  \bigskip
  
  What do we get if we put a prior over latent continuous valued regressors?
  }

\frame{
  Regression:
  $$y|X,\beta \sim \mbox{Normal}(X\beta^T,\sigma^2)$$

  Multivariate extension:

  $$Y|X,\beta \sim \mbox{Normal}(X\beta^T, \sigma^2 I)$$

  \pause

  Replace observed $X$ with latent variables $F$...

  $$\begin{aligned}
    F \sim& \mbox{Normal}(0, \sigma_Z^2I)\\
    \Lambda \sim& \mbox{Normal}(0,\sigma_\Lambda^2)\\
    Y|F,\Lambda \sim& \mbox{Normal}(F\Lambda^T, \sigma_Y^2)
  \end{aligned}$$

  What is $p(y_i |\Lambda, \sigma_Y^2)$?
}

\frame{
  $$
  \begin{aligned}
    p(y_i|\Lambda) =& \int p(y_i|f_i,\Lambda)p(f_i) df_i\\
    \propto& \int \exp\left\{-\frac{1}{2\sigma_y^2}(y_i - \Lambda f_i)^T(y_i -\Lambda f_i)\right\}\exp\left\{-\frac{1}{2}f_i^Tf_i\right\} df_i\\
    =& \int\exp \left\{-\frac{1}{2}\left((y_i - \Lambda f_i)^T(y_i -\Lambda f_i)/\sigma_y^2 + f_i^Tf_i\right)\right\}\\
    =& \int \exp \left\{ -\frac{1}{2}\left( \frac{y_i^Ty_i - f_i^T\Lambda^Ty_i - y_i^T\Lambda f_i + f_i^T\Lambda \Lambda^Tf_i}{\sigma_y^2}   f_i^Tf_i\right)\right\} df_i\\ 
    =& \int \exp\left\{ -\frac{1}{2}(f_i - m)^T\Sigma^{-1}(f_i-m) + y_i^T(\sigma_y^{2}I +\Lambda\Lambda^T)^{-1}y_i\right\}df_i\\
    &\left(\mbox{where } \Sigma = (I+\Lambda\Lambda^T/\sigma_y^2)^{-1}, m=\Sigma \Lambda^Ty_i/\sigma_y^2\right)\\
    \propto& \exp\left\{-\frac{1}{2}y_i^T(\sigma_y^2I + \Lambda \Lambda^T)^{-1}y_i\right\}
  \end{aligned}$$

So, $y_i|\Lambda, \sigma^2 \sim \mbox{Normal}(0,\sigma_yI + \Lambda \Lambda^T)$
}

  \frame{
    For a Gibbs sampler, we need the conditional distributions $p(F|\Lambda, Y)$ and $p(\Lambda|F,Y)$.

    $$\begin{aligned}
      p(\lambda_d|y,F) \propto& p(y^d|F,\Lambda_d) p(\Lambda_d)\\
      \propto& \exp\left\{-\frac{1}{2}\left(\frac{(y^d - F\Lambda_d)^T(y^d -F\Lambda_d)}{\sigma_y^2} - \frac{\Lambda_k^T\Lambda_k}{\sigma_\Lambda^2}\right)\right\}\\
    \end{aligned}$$

    \pause
    \bigskip
    
    Look familiar?? Conditioned on $F$, we have a linear regression model!

    \bigskip

    $$\lambda_d | y^d, F \sim \mbox{Normal}(m,S)$$
    where
    $$\begin{aligned}
      S =& \left(\sigma_\lambda^{-2}I + \sigma_y^{-2}F^TF\right)^{-1}
      m =& S F^Ty^d
      \end{aligned}$$
  }

  \frame{
    What about $p(f_i|\Lambda,Y)$?

    \pause
    \bigskip
    Well, our model is symmetric... conditioned on $\Lambda$, we have what looks like a regression:

    $$\begin{aligned}p(f_i|y_i,\Lambda) \propto& p(y_i|f_i,\Lambda)p(f_i)\\
      \propto \exp\left\{-\frac{1}{2}\left(\frac{(y_i-\Lambda f_i)^T(y_i - \Lambda f_i)}{\sigma_y^2} + f_i^Tf_i\right)\right\}\end{aligned}$$

      So,

      $$f_i \sim \mbox{Normal}(m, S)$$
       where
    $$\begin{aligned}
      S =& \left(I + \sigma_y^{-2}\Lambda^T\Lambda\right)^{-1}
      m =& S \Lambda^Ty_i
       \end{aligned}$$

       \pause
       Let's take a look at what this looks like! [notebook]
  }

  \frame{
    \begin{itemize}
    \item We saw, in our outputs, evidence of a lack of identifiability.
    \item The two solutions are equally ``good''.
    \item If we have an orthogonal transform $H$ such that $HH^T = H^TH = I$, then we can write

      $$Y = \Lambda F + \epsilon  \Lambda HH^TF + \epsilon = \tilde{\Lambda}\Tilde{F} + \epsilon$$

    \item $F$ and $\tilde{F}$ have the same statistical properties:
      $$\begin{aligned}
      E[\tilde{F}] =& H^TE[F] = 0\\
      cov(\tilde{F}) =& H^Tcov(F)H = H^TH=I\end{aligned}$$
    \end{itemize}
  }

  \frame{
    Netflix problem...
    \begin{tabular}{c | c c c c c }
      & Iron Man & Avengers & The Notebook & It & Saw \\
      \hline
      James & 3 & 2 & 1 & 5 & 5\\
      Joe & 4 & 5& 4 & 1 & 1\\
      Anna & 1 & ? & 2 & 5 & 4 \\
      Beth & 4 & 4 & 3 & 2 & 1
    \end{tabular}

    \bigskip

    How could we model this?
    }
\end{document}
